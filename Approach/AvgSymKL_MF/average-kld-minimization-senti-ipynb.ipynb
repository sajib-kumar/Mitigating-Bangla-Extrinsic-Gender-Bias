{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11289381,"sourceType":"datasetVersion","datasetId":7025039},{"sourceId":335927,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":281178,"modelId":302073}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"032183f0c4944c66833aa9cc1336ba07":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"045bb26e09874cbdaa6510e88eb78777":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05c4448fd05d49c581090d1a1080583a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"09d12339b8d1441f910a280b1d4600eb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bd54451147944ab98a3663787cc74f5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f0f696f481044c38b0f8c56e3a7c7ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7e39e43082141c3941b5f9f97f6cf66","placeholder":"​","style":"IPY_MODEL_f5d3bcc4bf1f4dc5b39f78d6ab3577f2","value":"model.safetensors: 100%"}},"0fc0a5baa8f443db90fff568ff625071":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1004f2c8264b4c8688b410fd6faf2ba4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15dd6b25672140a5b0cab1c4bd39213c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"204f8cd0cf524012ba6d20e06d8ec6a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f4cbdcdad25749469f956b17e6d48103","IPY_MODEL_c54c8cf872964e16ad8f059e710e73c7","IPY_MODEL_b223554875e9462aa2b4e3302549069a"],"layout":"IPY_MODEL_48be8114a17049feb2a117e06a6a3ac0"}},"31cb805b219b49bf8ed3e77bd95e7be5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"329e18b92bdd4d5fb26dcead9c1446a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa0bd25ed98b4bc7a193d052a9781a15","placeholder":"​","style":"IPY_MODEL_50607ccc6766418aace6f98782e60824","value":" 315/315 [00:00&lt;00:00, 24.5kB/s]"}},"33dc44fafab944c0b7cbd6d7be6d716a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37ba7c708c344b8f9e1079e2e362f46f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ef2bed1bc28c4bb998551a2b009c5854","IPY_MODEL_b9824f02aeb249cfb0cf34f2b8d31e9e","IPY_MODEL_9efe0d000d464fcc8e834256a90d2e94"],"layout":"IPY_MODEL_d099007ed141406f916b76385bfe64bd"}},"3a17692384234b288ce619e8fecff248":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0bd54451147944ab98a3663787cc74f5","max":315,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b865146cdf954d8d922da329d2c4115b","value":315}},"3da7942ea3d7456a98683e1fafa88089":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42be8dd209fa4a39ade1e601399a7d43":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48be8114a17049feb2a117e06a6a3ac0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c24a51356b54de89189b7750e03c5e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"50607ccc6766418aace6f98782e60824":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d39f46b46ac4fcabe6786c8e6d2d498":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"622e1db78d774597acf85d15330effe8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b6d294d9eca4e3c8052820fb9f904b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1d2e6d596984674878a89bb4c5d5337","placeholder":"​","style":"IPY_MODEL_8717deafd9b849d5853d9f02fa10394d","value":"tokenizer_config.json: 100%"}},"6c046e00e88e43b29672d7f1b7bc5ffa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d514b1afe3b48bdaed7fc76814a2ac1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2957b7362434064a1ad3ccdcfc83f8f","placeholder":"​","style":"IPY_MODEL_a8e0e503f4204a8a97c17e7ac24598bc","value":" 125/125 [00:00&lt;00:00, 11.9kB/s]"}},"6dd5bd49f8f54e97a5ec0d434d685699":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e049b79ae80494a9359f7070e995328":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ed90977e15949ae980591a4f8b5a7a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6b6d294d9eca4e3c8052820fb9f904b2","IPY_MODEL_3a17692384234b288ce619e8fecff248","IPY_MODEL_329e18b92bdd4d5fb26dcead9c1446a3"],"layout":"IPY_MODEL_15dd6b25672140a5b0cab1c4bd39213c"}},"72fe8b54cd7b48ed8ac1c872a6f578fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac9c7754db1c435e9117a12a12ac6c83","max":2919627,"min":0,"orientation":"horizontal","style":"IPY_MODEL_05c4448fd05d49c581090d1a1080583a","value":2919627}},"7d1194608bc4422e915f38b0d7533ca3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_85232d30193b4dc38147b3d965540ccf","max":711447648,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d5368081b1ef4b449514a0a08783eb22","value":711447648}},"7e0d4f0af27840d48ed3c9798dc44709":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_82b9bb268c194b929088bb89dd56bb80","IPY_MODEL_72fe8b54cd7b48ed8ac1c872a6f578fd","IPY_MODEL_831d4b3ee431490597a4238703feecce"],"layout":"IPY_MODEL_1004f2c8264b4c8688b410fd6faf2ba4"}},"7f338718be2a43ec91f2b6f6100a559e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0f0f696f481044c38b0f8c56e3a7c7ca","IPY_MODEL_7d1194608bc4422e915f38b0d7533ca3","IPY_MODEL_ca229eb8338e4de59e675bca6ab2598c"],"layout":"IPY_MODEL_d26c2aa635d4487ea9becf88e62d14aa"}},"82b9bb268c194b929088bb89dd56bb80":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f115e99cb22e4da2a790520fb5cda7f9","placeholder":"​","style":"IPY_MODEL_31cb805b219b49bf8ed3e77bd95e7be5","value":"tokenizer.json: 100%"}},"831d4b3ee431490597a4238703feecce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eca6ea6cb5f24af6bd714d526b22426e","placeholder":"​","style":"IPY_MODEL_42be8dd209fa4a39ade1e601399a7d43","value":" 2.92M/2.92M [00:00&lt;00:00, 3.49MB/s]"}},"85232d30193b4dc38147b3d965540ccf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8717deafd9b849d5853d9f02fa10394d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88d1e2914ef144c6aab6cd61168a5ec9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e3de60a6569479f87802686e631a11c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3da7942ea3d7456a98683e1fafa88089","placeholder":"​","style":"IPY_MODEL_032183f0c4944c66833aa9cc1336ba07","value":"special_tokens_map.json: 100%"}},"90c7cbd87fde4f44b4fd88b94650e460":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9efe0d000d464fcc8e834256a90d2e94":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e049b79ae80494a9359f7070e995328","placeholder":"​","style":"IPY_MODEL_e1fd92acc1884ab4b8f1e5a8bcd2b5d7","value":" 883/883 [00:00&lt;00:00, 91.1kB/s]"}},"a25d9fe6a00143899a57160974354fdc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8e0e503f4204a8a97c17e7ac24598bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac9c7754db1c435e9117a12a12ac6c83":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1d2e6d596984674878a89bb4c5d5337":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b223554875e9462aa2b4e3302549069a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a25d9fe6a00143899a57160974354fdc","placeholder":"​","style":"IPY_MODEL_90c7cbd87fde4f44b4fd88b94650e460","value":" 996k/996k [00:00&lt;00:00, 46.2MB/s]"}},"b865146cdf954d8d922da329d2c4115b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b9824f02aeb249cfb0cf34f2b8d31e9e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c046e00e88e43b29672d7f1b7bc5ffa","max":883,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e795d24d4ff14a039bf4f72615458420","value":883}},"c2957b7362434064a1ad3ccdcfc83f8f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c54c8cf872964e16ad8f059e710e73c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3d19a5e95164c309baade3798575894","max":995526,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0fc0a5baa8f443db90fff568ff625071","value":995526}},"ca229eb8338e4de59e675bca6ab2598c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_da8f43d44172414d8534ea01793a3706","placeholder":"​","style":"IPY_MODEL_045bb26e09874cbdaa6510e88eb78777","value":" 711M/711M [00:02&lt;00:00, 266MB/s]"}},"cc5a4e5bb4b84d89bc00ff61ea26dd3e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8e3de60a6569479f87802686e631a11c","IPY_MODEL_f6ac5297356346f3966bca1cdfdabaca","IPY_MODEL_6d514b1afe3b48bdaed7fc76814a2ac1"],"layout":"IPY_MODEL_5d39f46b46ac4fcabe6786c8e6d2d498"}},"d099007ed141406f916b76385bfe64bd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d26c2aa635d4487ea9becf88e62d14aa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5368081b1ef4b449514a0a08783eb22":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"da8f43d44172414d8534ea01793a3706":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1fd92acc1884ab4b8f1e5a8bcd2b5d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e795d24d4ff14a039bf4f72615458420":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eca6ea6cb5f24af6bd714d526b22426e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef2bed1bc28c4bb998551a2b009c5854":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_88d1e2914ef144c6aab6cd61168a5ec9","placeholder":"​","style":"IPY_MODEL_6dd5bd49f8f54e97a5ec0d434d685699","value":"config.json: 100%"}},"f115e99cb22e4da2a790520fb5cda7f9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3d19a5e95164c309baade3798575894":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4cbdcdad25749469f956b17e6d48103":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_09d12339b8d1441f910a280b1d4600eb","placeholder":"​","style":"IPY_MODEL_33dc44fafab944c0b7cbd6d7be6d716a","value":"vocab.txt: 100%"}},"f5d3bcc4bf1f4dc5b39f78d6ab3577f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f6ac5297356346f3966bca1cdfdabaca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_622e1db78d774597acf85d15330effe8","max":125,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4c24a51356b54de89189b7750e03c5e5","value":125}},"f7e39e43082141c3941b5f9f97f6cf66":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa0bd25ed98b4bc7a193d052a9781a15":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Mode Selection\nSet finetuning = 1 if you want to finetune, or set finetuning = 0 if you want to just load the saved model to get inference.","metadata":{"id":"Xrl5jB-tRKUD"}},{"cell_type":"code","source":"finetuning = 0\nload_and_run = not finetuning","metadata":{"execution":{"iopub.status.busy":"2025-04-13T10:36:10.895829Z","iopub.execute_input":"2025-04-13T10:36:10.896112Z","iopub.status.idle":"2025-04-13T10:36:10.899805Z","shell.execute_reply.started":"2025-04-13T10:36:10.896089Z","shell.execute_reply":"2025-04-13T10:36:10.899018Z"},"id":"nHQlVDO4ROqQ","trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Load Data","metadata":{"id":"AUrUFIDPlRdQ"}},{"cell_type":"code","source":"import pandas as pd\n\ndata = pd.read_csv('/kaggle/input/gb-all-necessary-updated/train_test_senti_data.csv')\n# data = pd.read_csv('train_test_senti_data.csv')\ndata","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.status.busy":"2025-04-13T10:36:10.903373Z","iopub.execute_input":"2025-04-13T10:36:10.903871Z","iopub.status.idle":"2025-04-13T10:36:11.038366Z","shell.execute_reply.started":"2025-04-13T10:36:10.903846Z","shell.execute_reply":"2025-04-13T10:36:11.037600Z"},"id":"aqM0Tr_zlT-w","outputId":"6bff7bac-33d7-4684-c51e-f60d1bfc8b0a","trusted":true},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                                   male  \\\n0     সাইন্সের ছেলেরা আসলেই একটু অহংকারী ও দাম্বিক হ...   \n1     কামাল এর ছাত্র এটা? এই ছাগটাকে প্রায় নাটকেই নে...   \n2                                   পোলা টা খালি কান্দে   \n3     ছাত্র টা কে যতবার দেখলাম ঘুষি মারতে ইচ্ছে হইছে...   \n4     ছেলেরা এরকমই হয়। সে ম্যাথ বুঝাতে পারেনি, আর ছে...   \n...                                                 ...   \n4847                                     অপুরভ ভাই অসাম   \n4848                              সেই ছিল এটা শিহাব ভাই   \n4849                              সেই ছিল এটা শিহাব ভাই   \n4850                              সেই ছিল এটা শিহাব ভাই   \n4851                                 ভাইয়া তুমি আমার বস   \n\n                                                 female  _original_label  test  \n0     সাইন্সের মেয়েরা আসলেই একটু অহংকারী ও দাম্বিক হ...              0.0     0  \n1     মেহজাবিন এর ছাত্রী এটা? এই ছাগীটাকে প্রায় নাটক...              0.0     0  \n2                                  মাইয়া টা খালি কান্দে              0.0     0  \n3     ছাত্রী টা কে যতবার দেখলাম ঘুষি মারতে ইচ্ছে হইছ...              0.0     0  \n4     মেয়েরা এরকমই হয়। সে ম্যাথ বুঝাতে পারেনি, আর মে...              0.0     0  \n...                                                 ...              ...   ...  \n4847                                    রাইসা ভাবী অসাম              1.0     0  \n4848                           সেই ছিল এটা নূরজাহান আফা              1.0     0  \n4849                           সেই ছিল এটা সুস্মিতা বোন              1.0     0  \n4850                            সেই ছিল এটা গুলশান ভাবী              1.0     0  \n4851                                   আপু তুমি আমার বস              1.0     0  \n\n[4852 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>male</th>\n      <th>female</th>\n      <th>_original_label</th>\n      <th>test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>সাইন্সের ছেলেরা আসলেই একটু অহংকারী ও দাম্বিক হ...</td>\n      <td>সাইন্সের মেয়েরা আসলেই একটু অহংকারী ও দাম্বিক হ...</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>কামাল এর ছাত্র এটা? এই ছাগটাকে প্রায় নাটকেই নে...</td>\n      <td>মেহজাবিন এর ছাত্রী এটা? এই ছাগীটাকে প্রায় নাটক...</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>পোলা টা খালি কান্দে</td>\n      <td>মাইয়া টা খালি কান্দে</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ছাত্র টা কে যতবার দেখলাম ঘুষি মারতে ইচ্ছে হইছে...</td>\n      <td>ছাত্রী টা কে যতবার দেখলাম ঘুষি মারতে ইচ্ছে হইছ...</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ছেলেরা এরকমই হয়। সে ম্যাথ বুঝাতে পারেনি, আর ছে...</td>\n      <td>মেয়েরা এরকমই হয়। সে ম্যাথ বুঝাতে পারেনি, আর মে...</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4847</th>\n      <td>অপুরভ ভাই অসাম</td>\n      <td>রাইসা ভাবী অসাম</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4848</th>\n      <td>সেই ছিল এটা শিহাব ভাই</td>\n      <td>সেই ছিল এটা নূরজাহান আফা</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4849</th>\n      <td>সেই ছিল এটা শিহাব ভাই</td>\n      <td>সেই ছিল এটা সুস্মিতা বোন</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4850</th>\n      <td>সেই ছিল এটা শিহাব ভাই</td>\n      <td>সেই ছিল এটা গুলশান ভাবী</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4851</th>\n      <td>ভাইয়া তুমি আমার বস</td>\n      <td>আপু তুমি আমার বস</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4852 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# Train, Test, Validation Split\nWe are going to consider the data as test data if the prediction between original sentence and NER converted sentence become unequal.","metadata":{"id":"cEf6-9-SjUs-"}},{"cell_type":"code","source":"temp = data[data['test']!= 1]\ntest = data[data['test']== 1]\nvalidation = temp.groupby('_original_label', group_keys=False).apply(lambda x: x.sample(frac=0.2, random_state=101))\ntrain = temp.drop(validation.index)","metadata":{"execution":{"iopub.status.busy":"2025-04-13T10:36:11.039430Z","iopub.execute_input":"2025-04-13T10:36:11.039679Z","iopub.status.idle":"2025-04-13T10:36:11.056056Z","shell.execute_reply.started":"2025-04-13T10:36:11.039661Z","shell.execute_reply":"2025-04-13T10:36:11.055213Z"},"id":"owR4v6Pb0S0_","trusted":true},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2877993709.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  validation = temp.groupby('_original_label', group_keys=False).apply(lambda x: x.sample(frac=0.2, random_state=101))\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"train","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.status.busy":"2025-04-13T10:36:11.056794Z","iopub.execute_input":"2025-04-13T10:36:11.057111Z","iopub.status.idle":"2025-04-13T10:36:11.069870Z","shell.execute_reply.started":"2025-04-13T10:36:11.057085Z","shell.execute_reply":"2025-04-13T10:36:11.069128Z"},"id":"4y8UqE3BNMJu","outputId":"d17bcf9c-3949-4c75-8b8d-4f310ec0aa31","trusted":true},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                   male  \\\n0     সাইন্সের ছেলেরা আসলেই একটু অহংকারী ও দাম্বিক হ...   \n1     কামাল এর ছাত্র এটা? এই ছাগটাকে প্রায় নাটকেই নে...   \n2                                   পোলা টা খালি কান্দে   \n3     ছাত্র টা কে যতবার দেখলাম ঘুষি মারতে ইচ্ছে হইছে...   \n4     ছেলেরা এরকমই হয়। সে ম্যাথ বুঝাতে পারেনি, আর ছে...   \n...                                                 ...   \n4844             তাহমিদ ভাইয়ার অভিনয় খুব ভাল লেগেছে...?   \n4847                                     অপুরভ ভাই অসাম   \n4849                              সেই ছিল এটা শিহাব ভাই   \n4850                              সেই ছিল এটা শিহাব ভাই   \n4851                                 ভাইয়া তুমি আমার বস   \n\n                                                 female  _original_label  test  \n0     সাইন্সের মেয়েরা আসলেই একটু অহংকারী ও দাম্বিক হ...              0.0     0  \n1     মেহজাবিন এর ছাত্রী এটা? এই ছাগীটাকে প্রায় নাটক...              0.0     0  \n2                                  মাইয়া টা খালি কান্দে              0.0     0  \n3     ছাত্রী টা কে যতবার দেখলাম ঘুষি মারতে ইচ্ছে হইছ...              0.0     0  \n4     মেয়েরা এরকমই হয়। সে ম্যাথ বুঝাতে পারেনি, আর মে...              0.0     0  \n...                                                 ...              ...   ...  \n4844                 তিশা আপুর অভিনয় খুব ভাল লেগেছে...?              1.0     0  \n4847                                    রাইসা ভাবী অসাম              1.0     0  \n4849                           সেই ছিল এটা সুস্মিতা বোন              1.0     0  \n4850                            সেই ছিল এটা গুলশান ভাবী              1.0     0  \n4851                                   আপু তুমি আমার বস              1.0     0  \n\n[3105 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>male</th>\n      <th>female</th>\n      <th>_original_label</th>\n      <th>test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>সাইন্সের ছেলেরা আসলেই একটু অহংকারী ও দাম্বিক হ...</td>\n      <td>সাইন্সের মেয়েরা আসলেই একটু অহংকারী ও দাম্বিক হ...</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>কামাল এর ছাত্র এটা? এই ছাগটাকে প্রায় নাটকেই নে...</td>\n      <td>মেহজাবিন এর ছাত্রী এটা? এই ছাগীটাকে প্রায় নাটক...</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>পোলা টা খালি কান্দে</td>\n      <td>মাইয়া টা খালি কান্দে</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ছাত্র টা কে যতবার দেখলাম ঘুষি মারতে ইচ্ছে হইছে...</td>\n      <td>ছাত্রী টা কে যতবার দেখলাম ঘুষি মারতে ইচ্ছে হইছ...</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ছেলেরা এরকমই হয়। সে ম্যাথ বুঝাতে পারেনি, আর ছে...</td>\n      <td>মেয়েরা এরকমই হয়। সে ম্যাথ বুঝাতে পারেনি, আর মে...</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4844</th>\n      <td>তাহমিদ ভাইয়ার অভিনয় খুব ভাল লেগেছে...?</td>\n      <td>তিশা আপুর অভিনয় খুব ভাল লেগেছে...?</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4847</th>\n      <td>অপুরভ ভাই অসাম</td>\n      <td>রাইসা ভাবী অসাম</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4849</th>\n      <td>সেই ছিল এটা শিহাব ভাই</td>\n      <td>সেই ছিল এটা সুস্মিতা বোন</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4850</th>\n      <td>সেই ছিল এটা শিহাব ভাই</td>\n      <td>সেই ছিল এটা গুলশান ভাবী</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4851</th>\n      <td>ভাইয়া তুমি আমার বস</td>\n      <td>আপু তুমি আমার বস</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3105 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"validation","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.status.busy":"2025-04-13T10:36:11.071634Z","iopub.execute_input":"2025-04-13T10:36:11.071886Z","iopub.status.idle":"2025-04-13T10:36:11.092662Z","shell.execute_reply.started":"2025-04-13T10:36:11.071866Z","shell.execute_reply":"2025-04-13T10:36:11.092163Z"},"id":"QxQqmLk8NQDi","outputId":"53e2c98e-4633-4ad8-d974-014b64e8403c","trusted":true},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                   male  \\\n955   নিশো ভাই এ নাটকটির মতই ইদানিং কিছু ফালতু ডিরেক...   \n171   পুরাই ফাউল নাটক বানাইছে।এসব গল্প কিভাবে লিখে স...   \n56    ভাই কাচ্চির এত অভাব কেউ কাচ্চি খুঁজে পেল না। ন...   \n127   এরকম বিশ্রী এবং অশ্লীল নাটক বন্ধ করুন এখানে যা...   \n683   শালা পরিচালক একটা লুচ্চা  যারা অভিনয় করছে তারা...   \n...                                                 ...   \n2765  আরিয়ান ভাইয়ের নাটক গুলো অনেক ভালো, আর নিশো ভাই...   \n2258  ভাল লাগছে..  .আমার প্রিয় নায়ক ..   আরফান নিশু....   \n3342  অসাধারন। আরিয়ান ভাই  এতো সুন্দর  নাটক  উপহার দ...   \n3322  মিজানুর রহমান আরিয়ান ভাই ধন্যবাদ।।।।।অপূর্ব তো...   \n4418  মোশারফ ভাইয়ের এবারের সবকটি নাটক খুবই ভাল লাগছে...   \n\n                                                 female  _original_label  test  \n955   রাইসা বোন এ নাটকটির মতই ইদানিং কিছু ফালতু ডিরে...              0.0     0  \n171   পুরাই ফাউল নাটক বানাইছে।এসব গল্প কিভাবে লিখে স...              0.0     0  \n56    ভাবী কাচ্চির এত অভাব কেউ কাচ্চি খুঁজে পেল না। ...              0.0     0  \n127   এরকম বিশ্রী এবং অশ্লীল নাটক বন্ধ করুন এখানে যা...              0.0     0  \n683   শালী পরিচালিকা একটা লুচ্চা যারা অভিনয় করছে তার...              0.0     0  \n...                                                 ...              ...   ...  \n2765  জাহান আপুর নাটক গুলো অনেক ভালো, আর কালীতারাভাব...              1.0     0  \n2258  ভাল লাগছে.. .আমার প্রিয় নায়িকা .. আদৃতা ... আফ...              1.0     0  \n3342  অসাধারন। আয়েশা বোন এতো সুন্দর নাটক উপহার দেয়ার...              1.0     0  \n3322  ফারহানা আফা ধন্যবাদ।।।।।অপূর্ব তো সব সময় অপূর্...              1.0     0  \n4418  বিনিতা ভাবীর এবারের সবকটি নাটক খুবই ভাল লাগছে।...              1.0     0  \n\n[777 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>male</th>\n      <th>female</th>\n      <th>_original_label</th>\n      <th>test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>955</th>\n      <td>নিশো ভাই এ নাটকটির মতই ইদানিং কিছু ফালতু ডিরেক...</td>\n      <td>রাইসা বোন এ নাটকটির মতই ইদানিং কিছু ফালতু ডিরে...</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>171</th>\n      <td>পুরাই ফাউল নাটক বানাইছে।এসব গল্প কিভাবে লিখে স...</td>\n      <td>পুরাই ফাউল নাটক বানাইছে।এসব গল্প কিভাবে লিখে স...</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>ভাই কাচ্চির এত অভাব কেউ কাচ্চি খুঁজে পেল না। ন...</td>\n      <td>ভাবী কাচ্চির এত অভাব কেউ কাচ্চি খুঁজে পেল না। ...</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>127</th>\n      <td>এরকম বিশ্রী এবং অশ্লীল নাটক বন্ধ করুন এখানে যা...</td>\n      <td>এরকম বিশ্রী এবং অশ্লীল নাটক বন্ধ করুন এখানে যা...</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>683</th>\n      <td>শালা পরিচালক একটা লুচ্চা  যারা অভিনয় করছে তারা...</td>\n      <td>শালী পরিচালিকা একটা লুচ্চা যারা অভিনয় করছে তার...</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2765</th>\n      <td>আরিয়ান ভাইয়ের নাটক গুলো অনেক ভালো, আর নিশো ভাই...</td>\n      <td>জাহান আপুর নাটক গুলো অনেক ভালো, আর কালীতারাভাব...</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2258</th>\n      <td>ভাল লাগছে..  .আমার প্রিয় নায়ক ..   আরফান নিশু....</td>\n      <td>ভাল লাগছে.. .আমার প্রিয় নায়িকা .. আদৃতা ... আফ...</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3342</th>\n      <td>অসাধারন। আরিয়ান ভাই  এতো সুন্দর  নাটক  উপহার দ...</td>\n      <td>অসাধারন। আয়েশা বোন এতো সুন্দর নাটক উপহার দেয়ার...</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3322</th>\n      <td>মিজানুর রহমান আরিয়ান ভাই ধন্যবাদ।।।।।অপূর্ব তো...</td>\n      <td>ফারহানা আফা ধন্যবাদ।।।।।অপূর্ব তো সব সময় অপূর্...</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4418</th>\n      <td>মোশারফ ভাইয়ের এবারের সবকটি নাটক খুবই ভাল লাগছে...</td>\n      <td>বিনিতা ভাবীর এবারের সবকটি নাটক খুবই ভাল লাগছে।...</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>777 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"test","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":981},"execution":{"iopub.status.busy":"2025-04-13T10:36:11.093534Z","iopub.execute_input":"2025-04-13T10:36:11.093786Z","iopub.status.idle":"2025-04-13T10:36:11.108282Z","shell.execute_reply.started":"2025-04-13T10:36:11.093766Z","shell.execute_reply":"2025-04-13T10:36:11.107561Z"},"id":"YHS3TElIORBB","outputId":"0bf3d650-1330-4cc6-d4a4-3681139908a2","trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                   male  \\\n10                              ছাত্র পড়ানো কঠিন প্যারা   \n11    ছেলেরা টেকায় না পড়লে কখনো ছেলেদের কাছে জায় না,...   \n13                                    পোলা খোর কখন আসবো   \n14                                    পোলা খোর কখন আসবো   \n17    মোশাররফ করিম ভাইয়া দিন দিন এতো বাজে নাটক করে ন...   \n...                                                 ...   \n4821                    শিহাব শাহীন ভাইকে অনেক ধন্যবাদ।   \n4827  শিহাব শাহিন এবং অপূর্ব মানে অন্যে রকম। তার মাঝ...   \n4836  সত্যিই সিহাব ভাইজান তুমার প্রতি দোয়া রইরো আমাদ...   \n4845                                     অপুরভ ভাই অসাম   \n4846                                     অপুরভ ভাই অসাম   \n\n                                                 female  _original_label  test  \n10                             ছাত্রী পড়ানো কঠিন প্যারা              0.0     1  \n11    মেয়েরা টেকায় না পড়লে কখনো মেয়েদের কাছে জায় না,...              0.0     1  \n13                                   মাইয়া খোর কখন আসবো              0.0     1  \n14                                    মেয়া খোর কখন আসবো              0.0     1  \n17      আয়েশা আপু দিন দিন এতো বাজে নাটক করে নিম্মমানের।              0.0     1  \n...                                                 ...              ...   ...  \n4821                         সাহানা বোনকে অনেক ধন্যবাদ।              1.0     1  \n4827  শিহাব শাহিন এবং রাইসা মানে অন্যে রকম। তার মাঝে...              1.0     1  \n4836  সত্যিই সিমন্তী বুবুজান তুমার প্রতি দোয়া রইরো আ...              1.0     1  \n4845                                   মুসফিকা আফা অসাম              1.0     1  \n4846                                    গুলশান বোন অসাম              1.0     1  \n\n[970 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>male</th>\n      <th>female</th>\n      <th>_original_label</th>\n      <th>test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>ছাত্র পড়ানো কঠিন প্যারা</td>\n      <td>ছাত্রী পড়ানো কঠিন প্যারা</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>ছেলেরা টেকায় না পড়লে কখনো ছেলেদের কাছে জায় না,...</td>\n      <td>মেয়েরা টেকায় না পড়লে কখনো মেয়েদের কাছে জায় না,...</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>পোলা খোর কখন আসবো</td>\n      <td>মাইয়া খোর কখন আসবো</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>পোলা খোর কখন আসবো</td>\n      <td>মেয়া খোর কখন আসবো</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>মোশাররফ করিম ভাইয়া দিন দিন এতো বাজে নাটক করে ন...</td>\n      <td>আয়েশা আপু দিন দিন এতো বাজে নাটক করে নিম্মমানের।</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4821</th>\n      <td>শিহাব শাহীন ভাইকে অনেক ধন্যবাদ।</td>\n      <td>সাহানা বোনকে অনেক ধন্যবাদ।</td>\n      <td>1.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4827</th>\n      <td>শিহাব শাহিন এবং অপূর্ব মানে অন্যে রকম। তার মাঝ...</td>\n      <td>শিহাব শাহিন এবং রাইসা মানে অন্যে রকম। তার মাঝে...</td>\n      <td>1.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4836</th>\n      <td>সত্যিই সিহাব ভাইজান তুমার প্রতি দোয়া রইরো আমাদ...</td>\n      <td>সত্যিই সিমন্তী বুবুজান তুমার প্রতি দোয়া রইরো আ...</td>\n      <td>1.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4845</th>\n      <td>অপুরভ ভাই অসাম</td>\n      <td>মুসফিকা আফা অসাম</td>\n      <td>1.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4846</th>\n      <td>অপুরভ ভাই অসাম</td>\n      <td>গুলশান বোন অসাম</td>\n      <td>1.0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>970 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# Minimizing KL Divergence","metadata":{"id":"Orwc8qgeow7g"}},{"cell_type":"markdown","source":"## Load Pretrained Model","metadata":{"id":"O27YmVxK7U2J"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transform\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM, BertConfig\nimport pandas as pd\nimport numpy as np\nimport transformers\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchsummary import summary\nfrom tqdm import tqdm\nfrom tqdm import tqdm\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nfrom datetime import datetime\nfrom transformers import get_linear_schedule_with_warmup\n\ndef load_transformer_based_model(model_path):\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForMaskedLM.from_pretrained(model_path ,output_hidden_states=True)\n    return tokenizer, model\nmodel_path = \"ka05ar/banglabert-sentiment\"\ntokenizer, b_model = load_transformer_based_model(model_path)\nprint('Total Pretrained tokens: ',len(tokenizer.get_vocab()))\ntokenizer.add_tokens(['<Name>' ,'<Gender>'])\nb_model.resize_token_embeddings(len(tokenizer))\nprint('Total number of tokens after adding the new ones: ',len(tokenizer.get_vocab()))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":429,"referenced_widgets":["6ed90977e15949ae980591a4f8b5a7a7","6b6d294d9eca4e3c8052820fb9f904b2","3a17692384234b288ce619e8fecff248","329e18b92bdd4d5fb26dcead9c1446a3","15dd6b25672140a5b0cab1c4bd39213c","b1d2e6d596984674878a89bb4c5d5337","8717deafd9b849d5853d9f02fa10394d","0bd54451147944ab98a3663787cc74f5","b865146cdf954d8d922da329d2c4115b","fa0bd25ed98b4bc7a193d052a9781a15","50607ccc6766418aace6f98782e60824","204f8cd0cf524012ba6d20e06d8ec6a0","f4cbdcdad25749469f956b17e6d48103","c54c8cf872964e16ad8f059e710e73c7","b223554875e9462aa2b4e3302549069a","48be8114a17049feb2a117e06a6a3ac0","09d12339b8d1441f910a280b1d4600eb","33dc44fafab944c0b7cbd6d7be6d716a","f3d19a5e95164c309baade3798575894","0fc0a5baa8f443db90fff568ff625071","a25d9fe6a00143899a57160974354fdc","90c7cbd87fde4f44b4fd88b94650e460","7e0d4f0af27840d48ed3c9798dc44709","82b9bb268c194b929088bb89dd56bb80","72fe8b54cd7b48ed8ac1c872a6f578fd","831d4b3ee431490597a4238703feecce","1004f2c8264b4c8688b410fd6faf2ba4","f115e99cb22e4da2a790520fb5cda7f9","31cb805b219b49bf8ed3e77bd95e7be5","ac9c7754db1c435e9117a12a12ac6c83","05c4448fd05d49c581090d1a1080583a","eca6ea6cb5f24af6bd714d526b22426e","42be8dd209fa4a39ade1e601399a7d43","cc5a4e5bb4b84d89bc00ff61ea26dd3e","8e3de60a6569479f87802686e631a11c","f6ac5297356346f3966bca1cdfdabaca","6d514b1afe3b48bdaed7fc76814a2ac1","5d39f46b46ac4fcabe6786c8e6d2d498","3da7942ea3d7456a98683e1fafa88089","032183f0c4944c66833aa9cc1336ba07","622e1db78d774597acf85d15330effe8","4c24a51356b54de89189b7750e03c5e5","c2957b7362434064a1ad3ccdcfc83f8f","a8e0e503f4204a8a97c17e7ac24598bc","37ba7c708c344b8f9e1079e2e362f46f","ef2bed1bc28c4bb998551a2b009c5854","b9824f02aeb249cfb0cf34f2b8d31e9e","9efe0d000d464fcc8e834256a90d2e94","d099007ed141406f916b76385bfe64bd","88d1e2914ef144c6aab6cd61168a5ec9","6dd5bd49f8f54e97a5ec0d434d685699","6c046e00e88e43b29672d7f1b7bc5ffa","e795d24d4ff14a039bf4f72615458420","6e049b79ae80494a9359f7070e995328","e1fd92acc1884ab4b8f1e5a8bcd2b5d7","7f338718be2a43ec91f2b6f6100a559e","0f0f696f481044c38b0f8c56e3a7c7ca","7d1194608bc4422e915f38b0d7533ca3","ca229eb8338e4de59e675bca6ab2598c","d26c2aa635d4487ea9becf88e62d14aa","f7e39e43082141c3941b5f9f97f6cf66","f5d3bcc4bf1f4dc5b39f78d6ab3577f2","85232d30193b4dc38147b3d965540ccf","d5368081b1ef4b449514a0a08783eb22","da8f43d44172414d8534ea01793a3706","045bb26e09874cbdaa6510e88eb78777"]},"execution":{"iopub.status.busy":"2025-04-13T10:36:11.108939Z","iopub.execute_input":"2025-04-13T10:36:11.109181Z","iopub.status.idle":"2025-04-13T10:36:18.718306Z","shell.execute_reply.started":"2025-04-13T10:36:11.109155Z","shell.execute_reply":"2025-04-13T10:36:18.717606Z"},"id":"YA0CO0ocCDP8","outputId":"ad713ceb-dd07-444c-b9e3-da23b898592c","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/430 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"920a4ce4481b4f5c852067c78396528f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/528k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6992733195c447d82783a65cdf698ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53bac73a37404cceba8ada8dccf39dd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e049ea211e9489ba99293658f3ecd5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/999 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"759a4c46ef324ec0b017fee4a653efc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3b8701a9f6047c78a43b7c90f04bb4a"}},"metadata":{}},{"name":"stderr","text":"Some weights of ElectraForMaskedLM were not initialized from the model checkpoint at ka05ar/banglabert-sentiment and are newly initialized: ['generator_lm_head.bias', 'generator_predictions.LayerNorm.bias', 'generator_predictions.LayerNorm.weight', 'generator_predictions.dense.bias', 'generator_predictions.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Total Pretrained tokens:  32000\n","output_type":"stream"},{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"name":"stdout","text":"Total number of tokens after adding the new ones:  32002\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Creating Custom Data Loader","metadata":{"id":"s3-7sVOl7LHt"}},{"cell_type":"code","source":"class BertDataset(Dataset):\n    def __init__(self, data, tokenizer,max_length):\n        super(BertDataset, self).__init__()\n\n        self.train_csv= data\n        self.tokenizer=tokenizer\n        self.target=self.train_csv.iloc[:,2]\n        self.max_length=max_length\n\n    def __len__(self):\n        return len(self.train_csv)\n\n    def __getitem__(self, index):\n\n        text1 = self.train_csv.iloc[index,0] #0th column male text\n        text2 = self.train_csv.iloc[index,1] #1st column female text\n\n        inputs = self.tokenizer.batch_encode_plus(\n            [text1, text2] ,\n            pad_to_max_length=True,\n            add_special_tokens=True,\n            return_attention_mask=True,\n            max_length=self.max_length,\n            truncation = True\n        )\n        ids = inputs[\"input_ids\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        masks = inputs[\"attention_mask\"]\n\n        return torch.tensor(ids[0], dtype=torch.long), torch.tensor(token_type_ids[0], dtype=torch.long), torch.tensor(masks[0], dtype=torch.long), torch.tensor(ids[1], dtype=torch.long), torch.tensor(token_type_ids[1], dtype=torch.long), torch.tensor(masks[1], dtype=torch.long), torch.tensor(self.train_csv.iloc[index, 2], dtype=torch.long)\n\nBATCH_SIZE = 4\n\ndataset_train= BertDataset(train, tokenizer, max_length=512 )\ndataset_val= BertDataset( validation, tokenizer, max_length=512)\ndataset_test= BertDataset(test, tokenizer, max_length=512)\ntrain_loader=DataLoader(dataset=dataset_train,batch_size=BATCH_SIZE, shuffle=True)\nvalidation_loader=DataLoader(dataset=dataset_val,batch_size=BATCH_SIZE, shuffle=False)\ntest_loader=DataLoader(dataset=dataset_test,batch_size=BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2025-04-13T10:36:18.719196Z","iopub.execute_input":"2025-04-13T10:36:18.719763Z","iopub.status.idle":"2025-04-13T10:36:18.734508Z","shell.execute_reply.started":"2025-04-13T10:36:18.719738Z","shell.execute_reply":"2025-04-13T10:36:18.733878Z"},"id":"OC9hovLMEOuI","trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Creating Custom Model","metadata":{"id":"jke0Ap9RAHbM"}},{"cell_type":"code","source":"import numpy as np\nnp.random.seed()\nclass BERT(nn.Module):\n    def __init__(self, model):\n        super(BERT, self).__init__()\n        self.bert_model = model\n        # print(self.bert_model.bert.embeddings.word_embeddings.weight)\n        self.dropout = nn.Dropout(0.2)\n        self.out = nn.Linear(768, 2)\n\n    def forward(self,ids1, mask1, token_type_ids1, ids2, mask2, token_type_ids2, mode):\n        if mode == 'training':\n          ids = torch.vstack((ids1, ids2))\n          mask = torch.vstack((mask1, mask2))\n          token_type_ids = torch.vstack((token_type_ids1, token_type_ids2))\n          o2= self.bert_model(ids,mask,token_type_ids).hidden_states[0]\n          o2= o2.max(dim=1).values\n          male_embedding, female_embedding = torch.vsplit(o2, 2)\n          # print(original_embedding.shape, ner_embedding.shape)\n          o2_male = self.dropout(male_embedding)\n          out_male= self.out(o2_male)\n\n          o2_female = self.dropout(female_embedding)\n          out_female= self.out(o2_female)\n\n\n          \n          \n\n          return (out_male + out_female)/2, F.softmax(out_male, dim=-1), F.softmax(out_female, dim=-1)\n\n\n        if mode == 'inference':\n          o2= self.bert_model(ids1, mask1, token_type_ids1).hidden_states[0]\n          oe= o2.max(dim=1).values\n          o2 = self.dropout(oe)\n          out= self.out(o2)\n          return out, oe\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmasked_model = nn.DataParallel(BERT(b_model)).to(device)","metadata":{"execution":{"iopub.status.busy":"2025-04-13T10:36:18.735261Z","iopub.execute_input":"2025-04-13T10:36:18.735494Z","iopub.status.idle":"2025-04-13T10:36:18.924148Z","shell.execute_reply.started":"2025-04-13T10:36:18.735478Z","shell.execute_reply":"2025-04-13T10:36:18.923594Z"},"id":"m3-yXKpLGWyo","trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"#debug\ni1, t1,m1,i2,t2,m2,l = next(iter(train_loader))\ni1 = i1.to(device)\nt1 = t1.to(device)\nm1 = m1.to(device)\ni2 = i2.to(device)\nt2 = t2.to(device)\nm2 = m2.to(device)\nl = l.to(device)\nprint('Input Shape: ', i1.shape)\n\nlogits, male_softmax, female_softmax = masked_model(i1, t1,m1,i2,t2,m2, 'training')\nprint(logits.shape, male_softmax.shape, male_softmax.shape)\nlogits, embedding = masked_model(i1, t1,m1,None,None,None, 'inference')\nprint(logits.shape, embedding.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-04-13T10:36:18.924947Z","iopub.execute_input":"2025-04-13T10:36:18.925151Z","iopub.status.idle":"2025-04-13T10:36:19.388985Z","shell.execute_reply.started":"2025-04-13T10:36:18.925136Z","shell.execute_reply":"2025-04-13T10:36:19.388009Z"},"id":"-AQGpFmWklsW","outputId":"68913ebc-412c-4ea6-cd31-23d78605eef5","trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2700: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Input Shape:  torch.Size([4, 512])\ntorch.Size([4, 2]) torch.Size([4, 2]) torch.Size([4, 2])\ntorch.Size([4, 2]) torch.Size([4, 768])\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"logits, logits.max(1)[1].view(-1,1)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-04-13T10:36:19.391446Z","iopub.execute_input":"2025-04-13T10:36:19.391652Z","iopub.status.idle":"2025-04-13T10:36:19.582023Z","shell.execute_reply.started":"2025-04-13T10:36:19.391635Z","shell.execute_reply":"2025-04-13T10:36:19.581460Z"},"id":"tnFND05I-A-D","outputId":"63115072-dbfe-4ce3-c30c-c17da03ab0d2","trusted":true},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(tensor([[-0.4761, -0.6282],\n         [-0.5259,  0.3777],\n         [-0.6271,  0.2891],\n         [-0.2105,  0.0419]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0],\n         [1],\n         [1],\n         [1]], device='cuda:0'))"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"logits.max(1)[1].view(-1,1).eq(l.view(-1,1)).sum().item()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-04-13T10:36:19.582840Z","iopub.execute_input":"2025-04-13T10:36:19.583048Z","iopub.status.idle":"2025-04-13T10:36:19.702350Z","shell.execute_reply.started":"2025-04-13T10:36:19.583032Z","shell.execute_reply":"2025-04-13T10:36:19.701807Z"},"id":"6JzJVLkuFHcw","outputId":"0f7415d8-3db7-439e-9594-4d0c720e3b8a","trusted":true},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"3"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"## Creating Utility Class for Tracking Training Stat","metadata":{"id":"WqKp5W2QTeMY"}},{"cell_type":"code","source":"from collections import OrderedDict\nfrom collections import namedtuple\nfrom itertools import product\nimport json\nfrom torch.utils.tensorboard import SummaryWriter\nimport time\nimport torchvision\nimport pandas as pd\nimport torch\nfrom IPython.display import display\nfrom IPython.display import clear_output\n\nclass RunBuilder():\n    @staticmethod\n    def get_runs(params):\n        Run = namedtuple('Run', params.keys())\n\n\n        runs = []\n        for v in product(*params.values()):\n            runs.append(Run(*v))\n\n\n        return runs\n\n\nclass RunManager():\n    def __init__(self):\n        self.epoch_count = 0\n        self.epoch_ce_loss = 0\n        self.epoch_kl_loss = 0\n        self.epoch_loss = 0\n        self.epoch_num_correct = 0\n        self.epoch_start_time = None\n\n        self.epoch_valid_ce_loss = 0.0\n        self.epoch_valid_kl_loss = 0.0\n        self.epoch_valid_loss = 0.0\n        self.epoch_num_valid_correct = 0\n\n        self.run_params = None\n        self.run_count = 0\n        self.run_data = []\n        self.run_start_time = None\n\n        self.network = None\n        self.loader = None\n        self.validation_loader = None\n        self.tb = None\n\n    def begin_run(self, run, network, loader, validation_loader):\n        self.run_start_time = time.time()\n\n        self.run_params = run\n        self.run_count += 1\n\n        self.network = network\n        self.loader = loader\n        self.validation_loader = validation_loader\n        self.tb = SummaryWriter(comment=f'-{run}')\n\n\n\n    def end_run(self):\n        self.tb.close()\n        self.epoch_count = 0\n\n    def begin_epoch(self):\n        self.epoch_start_time = time.time()\n        self.epoch_count += 1\n        self.epoch_loss = 0\n        self.epoch_ce_loss = 0\n        self.epoch_kl_loss = 0\n        self.epoch_num_correct = 0\n        self.epoch_valid_ce_loss = 0\n        self.epoch_valid_kl_loss = 0\n        self.epoch_valid_loss = 0\n        self.epoch_num_valid_correct = 0\n\n    def end_epoch(self):\n        epoch_duration = time.time() - self.epoch_start_time\n        run_duration = time.time() - self.run_start_time\n\n        ce_loss = self.epoch_ce_loss/len(self.loader.dataset)\n        kl_loss = self.epoch_kl_loss/len(self.loader.dataset)\n        loss = self.epoch_loss/len(self.loader.dataset)\n        accuracy = self.epoch_num_correct/len(self.loader.dataset)\n        val_ce_loss = self.epoch_valid_ce_loss/len(self.validation_loader.dataset)\n        val_kl_loss = self.epoch_valid_kl_loss/len(self.validation_loader.dataset)\n        val_loss = self.epoch_valid_loss/len(self.validation_loader.dataset)\n        val_accuracy = self.epoch_num_valid_correct/len(self.validation_loader.dataset)\n\n        self.tb.add_scalar('CE Loss', ce_loss, self.epoch_count)\n        self.tb.add_scalar('KL Loss', kl_loss, self.epoch_count)\n        self.tb.add_scalar('Loss', loss, self.epoch_count)\n        self.tb.add_scalar('Accuracy', accuracy, self.epoch_count)\n        self.tb.add_scalar('Validation CE Loss', val_ce_loss, self.epoch_count)\n        self.tb.add_scalar('Validation KL Loss', val_kl_loss, self.epoch_count)\n        self.tb.add_scalar('Validation Loss', val_loss, self.epoch_count)\n        self.tb.add_scalar('Validation Accuracy', val_accuracy, self.epoch_count)\n        print('-------------')\n        for name, param in self.network.named_parameters():\n            # print(name, param, self.epoch_count)\n            self.tb.add_histogram(name, param, self.epoch_count)\n\n        results = OrderedDict()\n        results[\"run\"] = self.run_count\n        results[\"epoch\"] = self.epoch_count\n        results[\"ce_loss\"] = ce_loss\n        results[\"kl_loss\"] = kl_loss\n        results[\"loss\"] = loss\n        results[\"accuracy\"] = accuracy\n        results[\"validation ce loss\"] = val_ce_loss\n        results[\"validation kl loss\"] = val_kl_loss\n        results[\"validation loss\"] = val_loss\n        results[\"validation accuracy\"] = val_accuracy\n        results[\"epoch duration (minutes)\"] = epoch_duration/60\n        results[\"run duration (minutes)\"] = run_duration/60\n        for k,v in self.run_params._asdict().items(): results[k] = v\n        self.run_data.append(results)\n        df = pd.DataFrame.from_dict(self.run_data, orient='columns')\n\n        clear_output(wait=True)\n        display(df)\n\n    def track_loss(self, ce_loss, kl_loss, loss):\n        self.epoch_ce_loss += ce_loss.item()#*self.loader.batch_size\n        self.epoch_kl_loss += kl_loss.item()#*self.loader.batch_size\n        self.epoch_loss += loss.item()#*self.loader.batch_size\n\n    def track_num_correct(self, preds, labels):\n        self.epoch_num_correct += self._get_num_correct(preds, labels)\n\n    def track_validation_loss(self, ce_loss, kl_loss, loss):\n        self.epoch_valid_ce_loss += ce_loss.item()#*self.loader.batch_size\n        self.epoch_valid_kl_loss += kl_loss.item()#*self.loader.batch_size\n        self.epoch_valid_loss += loss.item()#*self.loader.batch_size\n\n    def track_num_validation_correct(self, preds, labels):\n        self.epoch_num_valid_correct += self._get_num_correct(preds, labels)\n\n    @torch.no_grad()\n    def _get_num_correct(self, preds, labels):\n        return preds.max(1)[1].view(-1,1).eq(labels).sum().item()\n\n    def save(self, fileName):\n        pd.DataFrame.from_dict(\n            self.run_data,\n            orient='columns').to_csv(rf'{fileName}.csv')","metadata":{"execution":{"iopub.status.busy":"2025-04-13T10:36:19.703179Z","iopub.execute_input":"2025-04-13T10:36:19.703512Z","iopub.status.idle":"2025-04-13T10:36:19.721950Z","shell.execute_reply.started":"2025-04-13T10:36:19.703487Z","shell.execute_reply":"2025-04-13T10:36:19.721004Z"},"id":"T6W28-cwSnSV","trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Creating Training Loop","metadata":{"id":"dyXAASlbTyWQ"}},{"cell_type":"code","source":"SAVE_MODEL = True\ndef my_trainer(train_loader, validation_loader, network, LR = 1e-5 ,epochs = 5, BATCH_SIZE = 16,  fresh_training = True, validation = True):\n    params = OrderedDict(\n        lr = [LR],\n        batch_size =[BATCH_SIZE],\n        device = ['cuda' if torch.cuda.is_available() else 'cpu'],\n    )\n\n    m = RunManager()\n    for run in RunBuilder.get_runs(params):\n        device = torch.device(run.device)\n        # network = network.to(device)\n        # optimizer = optim.Adam(network.parameters(), lr=run.lr)\n\n        loss_fn = nn.CrossEntropyLoss()\n        loss_fn_2 = nn.KLDivLoss(reduction=\"batchmean\")\n\n        total_steps = len(train_loader) * epochs\n        optimizer = optim.Adam(network.parameters(), lr= run.lr)\n\n        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n        if not fresh_training:\n            load_model(network, optimizer, PATH = 'Senti_KLDM_Average.pth')\n        m.begin_run(run, network, train_loader, validation_loader)\n\n\n        for epoch in range(epochs):\n            network.train(True)\n            print(f'Epoch {epoch + 1} : ', end = '\\t')\n            m.begin_epoch()\n\n\n            for batch in tqdm(train_loader):\n                input_ids1, token_type_ids1, attention_masks1, input_ids2, token_type_ids2, attention_masks2, labels = batch\n                input_ids1 = input_ids1.to(device)\n                token_type_ids1 = token_type_ids1.to(device)\n                attention_masks1 = attention_masks1.to(device)\n\n                input_ids2 = input_ids2.to(device)\n                token_type_ids2 = token_type_ids2.to(device)\n                attention_masks2 = attention_masks2.to(device)\n\n                labels = labels.type(torch.LongTensor)\n                labels = labels.to(device)\n                copy_labels = labels\n\n                preds, male_softmax, female_softmax = network(input_ids1, attention_masks1, token_type_ids1, input_ids2, attention_masks2, token_type_ids2, 'training')\n                labels = labels.view(-1, 1)\n\n\n\n                loss1 = loss_fn(preds, copy_labels)\n                loss2 = loss_fn_2(male_softmax.log(), female_softmax) + loss_fn_2(female_softmax.log(), male_softmax)\n                loss = loss1 + loss2\n                # print(preds.shape, labels.shape)\n                # print(preds, labels)\n\n\n                optimizer.zero_grad()\n                loss.backward()\n\n                torch.nn.utils.clip_grad_norm_(network.parameters(), 1.0)\n\n                optimizer.step()\n\n                scheduler.step()\n\n\n                m.track_loss(loss1, loss2, loss)\n                m.track_num_correct(preds, labels)\n\n\n            network.eval()\n            # print(network.bert_model.bert.embeddings.word_embeddings.weight)\n            for batch in tqdm(validation_loader):\n                input_ids1, token_type_ids1, attention_masks1, input_ids2, token_type_ids2, attention_masks2, labels = batch\n                input_ids1 = input_ids1.to(device)\n                token_type_ids1 = token_type_ids1.to(device)\n                attention_masks1 = attention_masks1.to(device)\n                input_ids2 = input_ids2.to(device)\n                token_type_ids2 = token_type_ids2.to(device)\n                attention_masks2 = attention_masks2.to(device)\n\n                labels = labels.type(torch.LongTensor)\n                labels = labels.to(device)\n                copy_labels = labels\n                with torch.no_grad():\n                    preds, male_softmax, female_softmax = network(input_ids1, attention_masks1, token_type_ids1, input_ids2, attention_masks2, token_type_ids2, 'training')\n                    labels = labels.view(-1, 1)\n                    loss1 = loss_fn(preds, copy_labels)\n                    loss2 = loss_fn_2(male_softmax.log(), female_softmax) + loss_fn_2(female_softmax.log(),male_softmax)\n                    loss = loss1 + loss2\n                    m.track_validation_loss(loss1, loss2, loss)\n                    m.track_num_validation_correct(preds, labels)\n                # break #not doing validation here\n\n\n\n\n            m.end_epoch()\n            network.train(False)\n            save_model(network, optimizer)\n        m.end_run()\n    if SAVE_MODEL == True:\n        save_model(network, optimizer)\n\n    s = datetime.now().strftime(\"%d-%m-%Y %I:%M:%p\").replace(\" \", \"_\").replace(\":\", \"_\")\n    PATH  = rf'results_{s}'\n    m.save(PATH)\n    return\n\n\ndef save_model(network, optimizer):\n    s = datetime.now().strftime(\"%d-%m-%Y %I:%M:%p\").replace(\" \", \"_\").replace(\":\", \"_\")\n    PATH  = rf'Senti_KLDM_Average.pth'\n    torch.save({\n    'model_state_dict': network.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    }, PATH)\n    print(f'Saved model and optimizer at {PATH}')\n    return\n\ndef load_model(network, optimizer, PATH ):\n    checkpoint = torch.load(PATH)\n    network.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        print(rf'Loaded model and optimizer from {PATH}')\n    else:\n        print(rf'Loaded model from {PATH}')\n\n    return\n\ndef get_parameter_info(network, details = False):\n    print('Total parameters:' ,(sum(p.numel() for p in network.parameters())))\n    print('Total trainable parameters:' ,(sum(p.numel() for p in network.parameters() if p.requires_grad)))\n    if details:\n        print('---------------------------------\\nDetailed Parameter Info: ')\n\n        for name, parameter in network.named_parameters():\n            print('name, parameter.requires_grad, parameter.is_cuda, parameter.size(): ', name, parameter.requires_grad, parameter.is_cuda, parameter.size())\n            print('---------------------------------')\n    return\n\nif finetuning:\n  my_trainer(train_loader, validation_loader, masked_model, 0.0001, epochs = 15, BATCH_SIZE = BATCH_SIZE, fresh_training = True)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":786},"execution":{"iopub.status.busy":"2025-04-13T10:36:19.722816Z","iopub.execute_input":"2025-04-13T10:36:19.723034Z","iopub.status.idle":"2025-04-13T10:36:19.745486Z","shell.execute_reply.started":"2025-04-13T10:36:19.723018Z","shell.execute_reply":"2025-04-13T10:36:19.744823Z"},"id":"R6Y4KVbRSxRB","outputId":"17769375-cb29-4c4a-f774-dbf43154fd87","trusted":true},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Load Saved Model","metadata":{"id":"XPze9PybVSUi"}},{"cell_type":"code","source":"#Load Model\nfrom tqdm import tqdm\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nfrom datetime import datetime\nfrom transformers import get_linear_schedule_with_warmup\n\ndef load_model(network, optimizer, PATH ):\n    checkpoint = torch.load(PATH, map_location=torch.device('cpu'))\n    network.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        print(rf'Loaded model and optimizer from {PATH}')\n    else:\n        print(rf'Loaded model from {PATH}')\n\n    return\n\n\nif load_and_run:\n#   load_model(masked_model, None, PATH = '/kaggle/input/toxicity_kldm/pytorch/default/1/Toxicity_KLDM.pth')\n  load_model(masked_model, None, PATH = '/kaggle/input/senti_kldm_average/pytorch/default/1/Senti_KLDM_Average.pth')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-04-13T10:36:19.746178Z","iopub.execute_input":"2025-04-13T10:36:19.746462Z","iopub.status.idle":"2025-04-13T10:36:20.351529Z","shell.execute_reply.started":"2025-04-13T10:36:19.746426Z","shell.execute_reply":"2025-04-13T10:36:20.350899Z"},"id":"LD9moMLZLl8J","outputId":"51f62199-2009-4895-8ab9-aa4b561d0291","trusted":true},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2980459877.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(PATH, map_location=torch.device('cpu'))\n","output_type":"stream"},{"name":"stdout","text":"Loaded model from Senti_KLDM_Average.pth\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Inference","metadata":{"id":"BSizhcU-V_co"}},{"cell_type":"code","source":"import random\nimport warnings\nwarnings.filterwarnings('ignore')\ndef get_prediction(data_loader, network, choice = 'original'):\n  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n  device = torch.device(device)\n  network = network.to(device)\n  network.eval()\n  l = []\n  emb = []\n  for batch in tqdm(data_loader):\n    input_ids1, token_type_ids1, attention_masks1, input_ids2, token_type_ids2, attention_masks2, labels = batch\n    input_ids1 = input_ids1.to(device)\n    token_type_ids1 = token_type_ids1.to(device)\n    attention_masks1 = attention_masks1.to(device)\n    input_ids2 = input_ids2.to(device)\n    token_type_ids2 = token_type_ids2.to(device)\n    attention_masks2 = attention_masks2.to(device)\n    labels = labels.type(torch.LongTensor)\n    labels = labels.to(device)\n    with torch.no_grad():\n        if choice == 'male':\n          preds, hidden_states = network(input_ids1, attention_masks1,token_type_ids1, None, None, None, 'inference')\n        else:\n          preds, hidden_states = network(input_ids2, attention_masks2,token_type_ids2, None, None, None, 'inference')\n\n        labels = labels.type_as(preds).view(-1, 1)\n        preds =  preds.max(1)[1].view(-1,1)\n        l.append(preds)\n        emb.append(hidden_states)\n  return l, torch.vstack(emb).cpu().numpy()\n\np, emb_male = get_prediction(test_loader, masked_model, 'male')\ntest['pred_male_kldm'] = [x[0] for pred in p for x in pred.tolist()]\np, emb_male = get_prediction(test_loader, masked_model, 'female')\ntest['pred_female_kldm'] = [x[0] for pred in p for x in pred.tolist()]\nx = sum(test['pred_male_kldm']!=test['_original_label'])\nprint('For Male:\\nTotal test data: ', len(test),'Total mismatch: ', x,'Accuracy: ', 1- (x/len(test))) \nx = sum(test['pred_female_kldm']!=test['_original_label'])\nprint('For Female:\\nTotal test data: ', len(test),'Total mismatch: ', x,'Accuracy: ', 1- (x/len(test)))\nprint('Male Female Mismatch: ', sum(test['pred_male_kldm']!=test['pred_female_kldm']))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-04-13T10:36:20.352192Z","iopub.execute_input":"2025-04-13T10:36:20.352414Z","iopub.status.idle":"2025-04-13T10:37:01.228446Z","shell.execute_reply.started":"2025-04-13T10:36:20.352373Z","shell.execute_reply":"2025-04-13T10:37:01.227753Z"},"id":"YEiHQSXpUB_J","outputId":"f5d7ce9f-03e0-4057-ad92-2f507d6d914e","trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 243/243 [00:20<00:00, 11.94it/s]\n100%|██████████| 243/243 [00:20<00:00, 11.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"For Male:\nTotal test data:  970 Total mismatch:  49 Accuracy:  0.9494845360824742\nFor Female:\nTotal test data:  970 Total mismatch:  56 Accuracy:  0.9422680412371134\nMale Female Mismatch:  19\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# KLD Minimization Result","metadata":{}},{"cell_type":"code","source":"import numpy as np\nnp.random.seed(0)\n\ndef bootstrap_ci(y_true, y_pred, B=500):\n    \"\"\"Compute bootstrap confidence interval for accuracy.\"\"\"\n    N = len(y_true)\n    original_acc = np.mean(y_pred == y_true)\n\n    bootstrap_accs = []\n\n    for _ in range(B):\n        indices = np.random.choice(N, N, replace=True)  # Sample with replacement\n        y_sample = y_true[indices]\n        acc = np.mean(y_pred[indices] == y_sample)\n        bootstrap_accs.append(acc)\n    # Compute 95% confidence interval\n    lower, upper = np.percentile(bootstrap_accs, [2.5, 97.5])\n\n    return original_acc, (lower, upper), np.mean(bootstrap_accs)\n\ny_true = test['_original_label'].values  # Ground truth labels\ny_pred = test['pred_male_kldm'].values\nacc, ci, ma = bootstrap_ci(y_true, y_pred)\nprint('Male: ')\nprint(f\"Accuracy: {acc*100:.2f}\")\nprint(f\"95% Confidence Interval: {ci[0]*100:.2f}  {ci[1]*100:.2f}\")\nprint(f\"mean accuracy: {ma*100: .2f}\")\n\ny_true = test['_original_label'].values\ny_pred = test['pred_female_kldm'].values\nacc, ci, ma = bootstrap_ci(y_true, y_pred)\nprint('\\nFemale: ')\nprint(f\"Accuracy: {acc*100:.2f}\")\nprint(f\"95% Confidence Interval: {ci[0]*100:.2f}  {ci[1]*100:.2f}\")\nprint(f\"mean accuracy: {ma*100: .2f}\")\nprint('Male Female Mismatch: ', sum(test['pred_male_kldm']!=test['pred_female_kldm']))\ntest.to_csv('revision_kldm_average_senti_result.csv', index = False)\nprint('Saved....')","metadata":{"execution":{"iopub.status.busy":"2025-04-13T10:37:01.229241Z","iopub.execute_input":"2025-04-13T10:37:01.229492Z","iopub.status.idle":"2025-04-13T10:37:01.293379Z","shell.execute_reply.started":"2025-04-13T10:37:01.229466Z","shell.execute_reply":"2025-04-13T10:37:01.292828Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Male: \nAccuracy: 94.95\n95% Confidence Interval: 93.51  96.08\nmean accuracy:  94.93\n\nFemale: \nAccuracy: 94.23\n95% Confidence Interval: 92.73  95.67\nmean accuracy:  94.25\nMale Female Mismatch:  19\nSaved....\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Statistical Parity Difference (SPD) and Equal Opportunity Difference (EOD) Calculation\nimport numpy as np\nimport pandas as pd\nnp.random.seed(0)\n\ntest = pd.read_csv('revision_kldm_average_senti_result.csv')\n\n\n# Function to calculate Statistical Parity Difference (SPD)\ndef calculate_spd(male_pred, female_pred):\n    # Calculate probabilities of positive predictions for males and females\n    p_male = np.mean(male_pred == 1)  # Proportion of positive predictions for males\n    p_female = np.mean(female_pred == 1)  # Proportion of positive predictions for females\n    \n    # Return the absolute value of the Statistical Parity Difference\n    return np.abs(p_male - p_female)\n\n# Function to calculate Equal Opportunity Difference (EOD)\ndef calculate_eod(male_pred, female_pred, y_true):\n    # True Positive Rate for males\n    tpr_male = np.mean((male_pred == 1) & (y_true == 1))  # True positives for males\n    \n    # True Positive Rate for females\n    tpr_female = np.mean((female_pred == 1) & (y_true == 1))  # True positives for females\n    \n    # Return the absolute value of the Equal Opportunity Difference\n    return np.abs(tpr_male - tpr_female)\n\n# Function to calculate bootstrap confidence intervals for SPD and EOD\ndef bootstrap_ci_spd_eod(male_pred, female_pred, y_true, n_iterations=500, ci=95):\n    n = len(male_pred)\n    \n    # Arrays to store SPD and EOD for each bootstrap sample\n    spd_values = np.zeros(n_iterations)\n    eod_values = np.zeros(n_iterations)\n    \n    # Perform bootstrap sampling\n    for i in range(n_iterations):\n        # Resample with replacement\n        sample_indices = np.random.choice(n, size=n, replace=True)\n        male_pred_resampled = male_pred[sample_indices]\n        female_pred_resampled = female_pred[sample_indices]\n        y_true_resampled = y_true[sample_indices]\n        \n        # Calculate SPD and EOD for the resampled data\n        spd_values[i] = calculate_spd(male_pred_resampled, female_pred_resampled)\n        eod_values[i] = calculate_eod(male_pred_resampled, female_pred_resampled, y_true_resampled)\n    \n    # Calculate the mean of SPD and EOD\n    spd_mean = np.mean(spd_values)\n    eod_mean = np.mean(eod_values)\n    \n    # Calculate the confidence interval bounds for SPD and EOD\n    lower_percentile = (100 - ci) / 2\n    upper_percentile = 100 - lower_percentile\n    spd_lower_bound = np.percentile(spd_values, lower_percentile)\n    spd_upper_bound = np.percentile(spd_values, upper_percentile)\n    eod_lower_bound = np.percentile(eod_values, lower_percentile)\n    eod_upper_bound = np.percentile(eod_values, upper_percentile)\n    \n    return (np.abs(spd_mean), np.abs(spd_lower_bound), np.abs(spd_upper_bound)), (np.abs(eod_mean), np.abs(eod_lower_bound), np.abs(eod_upper_bound))\n\n\ny_true = test._original_label.values\nmale_pred = test.pred_male_kldm.values\nfemale_pred = test.pred_female_kldm.values\n\n# Calculate bootstrap CI and mean for SPD and EOD\n(spd_mean, spd_lower, spd_upper), (eod_mean, eod_lower, eod_upper) = bootstrap_ci_spd_eod(\n    male_pred, female_pred, y_true, n_iterations=500, ci=95\n)\n\nprint(f\"SPD 95% Confidence Interval: [{spd_lower:.3f}, {spd_upper:.3f}]\\tSPD Mean: {spd_mean:.3f}\")\nprint('---------------------------------------------------------------------------------------------')\nprint(f\"EOD 95% Confidence Interval: [{eod_lower:.3f}, {eod_upper:.3f}]\\tEOD Mean: {eod_mean:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:37:01.294028Z","iopub.execute_input":"2025-04-13T10:37:01.294220Z","iopub.status.idle":"2025-04-13T10:37:01.364043Z","shell.execute_reply.started":"2025-04-13T10:37:01.294205Z","shell.execute_reply":"2025-04-13T10:37:01.363423Z"}},"outputs":[{"name":"stdout","text":"SPD 95% Confidence Interval: [0.001, 0.016]\tSPD Mean: 0.007\n---------------------------------------------------------------------------------------------\nEOD 95% Confidence Interval: [0.000, 0.007]\tEOD Mean: 0.003\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Male: \n# Accuracy: 0.9043\n# 95% Confidence Interval: (np.float64(0.8882801418439716), np.float64(0.9198936170212766))\n# mean accuracy:  90.39\n\n# Female: \n# Accuracy: 0.8950\n# 95% Confidence Interval: (np.float64(0.8797695035460993), np.float64(0.9113475177304965))\n# mean accuracy:  89.50\n# Male Female Mismatch:  33\n# Saved....","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Male: \n# Accuracy: 0.9071\n# 95% Confidence Interval: (0.8918262411347517, 0.9223581560283688)\n# mean accuracy: 0.9068921985815602\n\n# Female: \n# Accuracy: 0.8965\n# 95% Confidence Interval: (0.8818971631205674, 0.9113475177304965)\n# mean accuracy: 0.896463829787234\n# Male Female Mismatch:  43\n# Saved....","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Result of Approach 2","metadata":{"id":"5aWHH7krrDB9"}},{"cell_type":"code","source":"x = sum(test['original_lebel'] != test['a2_original_pred'])\nprint('For Original text:\\nTotal test data: ', len(test),'Total mismatch: ', x,'Accuracy: ', 1 - (x/len(test)) )\nx = sum(test['original_lebel'] != test['a2_ner_pred'])\nprint('For NER Converted text:\\nTotal test data: ', len(test),'Total mismatch: ', x,'Accuracy: ', 1 - (x/len(test)))\nprint('Bias: ', sum(test['a2_ner_pred'] != test['a2_original_pred']))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-03-29T19:27:53.904314Z","iopub.status.idle":"2025-03-29T19:27:53.904742Z","shell.execute_reply":"2025-03-29T19:27:53.904572Z"},"id":"LJ8w7gWFTA_2","outputId":"f6bbc388-3858-46f6-85d4-d6d88e9678ed","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x = sum(test['original_lebel'] != test['a2_original_pred'])\nprint('For Original text:\\nTotal test data: ', len(test),'Total mismatch: ', x,'Accuracy: ', 1 - (x/len(test)) )\nx = sum(test['original_lebel'] != test['a2_ner_pred'])\nprint('For NER Converted text:\\nTotal test data: ', len(test),'Total mismatch: ', x,'Accuracy: ', 1 - (x/len(test)))\nprint('Bias: ', sum(test['a2_ner_pred'] != test['a2_original_pred']))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-03-29T19:27:53.905846Z","iopub.status.idle":"2025-03-29T19:27:53.906257Z","shell.execute_reply":"2025-03-29T19:27:53.906113Z"},"id":"EFdX-53TUFoM","outputId":"05f72853-37de-4559-96e4-5a5f1de83064","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x = test[test['a2_ner_pred'] != test['a2_original_pred']][['Original Sentence', 'Converted Sentence', 'original_lebel', 'a2_original_pred', 'a2_ner_pred']]\nprint(len(x))\nx","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.status.busy":"2025-03-29T19:27:53.907706Z","iopub.status.idle":"2025-03-29T19:27:53.908159Z","shell.execute_reply":"2025-03-29T19:27:53.907945Z"},"id":"zuG_GnlHPDWy","outputId":"d1427500-af23-47c6-d779-77316193ac4f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sentence Embedding Visualization for Original Sentence and NER Converted Sentence","metadata":{"id":"VgYmBLCUmz7v"}},{"cell_type":"markdown","source":"### For Training Data","metadata":{"id":"PRNFs3ZtQAA9"}},{"cell_type":"code","source":"#for visualizing training data embedding\ndataset_train= BertDataset(train, tokenizer, max_length=200)\ntrain_loader=DataLoader(dataset=dataset_train,batch_size=16, shuffle = False)\np, emb_ori = get_prediction(train_loader, masked_model, 'original')\np, emb_ner = get_prediction(train_loader, masked_model, 'ner')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-03-29T19:27:53.911058Z","iopub.status.idle":"2025-03-29T19:27:53.911765Z","shell.execute_reply":"2025-03-29T19:27:53.911341Z"},"id":"CHcPFENZgURv","outputId":"0fd83727-85ee-4f4d-bff7-5b4609407970","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport plotly.express as px\n\ntsne_cls = TSNE(n_components=2, random_state=42).fit_transform(emb_ori)\n\nlabels = train['original_lebel']   # Extract labels\nreview_texts = train['Original Sentence']  # Extract review texts\n\n# Prepare the data for Plotly plots\nplot_data_cls = {'t-SNE Dimension 1': tsne_cls[:, 0],\n                 't-SNE Dimension 2': tsne_cls[:, 1],\n                 'Label': labels,\n                 'Text': review_texts}\n\n# Customize colorbar to show integer labels\ncolorbar_tickvals = list(range(1, 6))\ncolorbar_ticktext = [str(val) for val in colorbar_tickvals]\n\n# Plotting with Plotly - [CLS] Token Embeddings\nfig_cls = px.scatter(plot_data_cls, x='t-SNE Dimension 1', y='t-SNE Dimension 2',\n                     color='Label', hover_data={'t-SNE Dimension 1': False, 't-SNE Dimension 2': False, 'Text': True})\n\n\nfig_cls.update_layout(title='t-SNE of Embeddings Original Training Data with Labels', height=700,  font=dict(\n        size=20,\n))\nfig_cls.update_traces(marker=dict(size=10, opacity=0.9))\n\nfig_cls.update_layout(coloraxis_colorbar=dict(\n    tickvals=colorbar_tickvals,\n    ticktext=colorbar_ticktext\n))\n\nfig_cls.show()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":717},"execution":{"iopub.status.busy":"2025-03-29T19:27:53.913097Z","iopub.status.idle":"2025-03-29T19:27:53.913617Z","shell.execute_reply":"2025-03-29T19:27:53.913389Z"},"id":"f701BSC2csbW","outputId":"3c3befde-cb42-407b-f22b-49d68d99fea5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport plotly.express as px\n\ntsne_cls = TSNE(n_components=2, random_state=42).fit_transform(emb_ner)\n\nlabels = train['original_lebel']   # Extract labels\nreview_texts = train['Converted Sentence']  # Extract review texts\n\n# Prepare the data for Plotly plots\nplot_data_cls = {'t-SNE Dimension 1': tsne_cls[:, 0],\n                 't-SNE Dimension 2': tsne_cls[:, 1],\n                 'Label': labels,\n                 'Text': review_texts}\n\n# Customize colorbar to show integer labels\ncolorbar_tickvals = list(range(1, 6))\ncolorbar_ticktext = [str(val) for val in colorbar_tickvals]\n\n# Plotting with Plotly - [CLS] Token Embeddings\nfig_cls = px.scatter(plot_data_cls, x='t-SNE Dimension 1', y='t-SNE Dimension 2',\n                     color='Label', hover_data={'t-SNE Dimension 1': False, 't-SNE Dimension 2': False, 'Text': True})\n\n\nfig_cls.update_layout(title='t-SNE of Embeddings NER Converted Training Data with Labels', height=700,  font=dict(\n        size=20,\n))\nfig_cls.update_traces(marker=dict(size=10, opacity=0.9))\n\nfig_cls.update_layout(coloraxis_colorbar=dict(\n    tickvals=colorbar_tickvals,\n    ticktext=colorbar_ticktext\n))\n\nfig_cls.show()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":717},"execution":{"iopub.status.busy":"2025-03-29T19:27:53.914955Z","iopub.status.idle":"2025-03-29T19:27:53.915388Z","shell.execute_reply":"2025-03-29T19:27:53.915198Z"},"id":"qu8AepygZpgq","outputId":"daa57041-9065-488c-b805-43a32c0600d2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### For Biased Test Data","metadata":{"id":"zkBhm7pRQM-3"}},{"cell_type":"code","source":"#for visualizing training data embedding\ndataset_train= BertDataset(x, tokenizer, max_length=200)\ntrain_loader=DataLoader(dataset=dataset_train,batch_size=16, shuffle = False)\np, emb_ori = get_prediction(train_loader, masked_model, 'original')\np, emb_ner = get_prediction(train_loader, masked_model, 'ner')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-03-29T19:27:53.916998Z","iopub.status.idle":"2025-03-29T19:27:53.917438Z","shell.execute_reply":"2025-03-29T19:27:53.917284Z"},"id":"vyV3an9LQQTl","outputId":"8f4e495d-7178-4623-98f5-9a404869b989","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport plotly.express as px\n\ntsne_cls = TSNE(n_components=2, random_state=42).fit_transform(emb_ori)\n\nlabels = x['original_lebel']   # Extract labels\nreview_texts = x['Original Sentence']  # Extract review texts\n\n# Prepare the data for Plotly plots\nplot_data_cls = {'t-SNE Dimension 1': tsne_cls[:, 0],\n                 't-SNE Dimension 2': tsne_cls[:, 1],\n                 'Label': labels,\n                 'Text': review_texts}\n\n# Customize colorbar to show integer labels\ncolorbar_tickvals = list(range(1, 6))\ncolorbar_ticktext = [str(val) for val in colorbar_tickvals]\n\n# Plotting with Plotly - [CLS] Token Embeddings\nfig_cls = px.scatter(plot_data_cls, x='t-SNE Dimension 1', y='t-SNE Dimension 2',\n                     color='Label', hover_data={'t-SNE Dimension 1': False, 't-SNE Dimension 2': False, 'Text': True})\n\n\nfig_cls.update_layout(title='t-SNE of Embeddings Original Training Data with Labels', height=700,  font=dict(\n        size=20,\n))\nfig_cls.update_traces(marker=dict(size=10, opacity=0.9))\n\nfig_cls.update_layout(coloraxis_colorbar=dict(\n    tickvals=colorbar_tickvals,\n    ticktext=colorbar_ticktext\n))\n\nfig_cls.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":717},"execution":{"iopub.status.busy":"2025-03-29T19:27:53.918325Z","iopub.status.idle":"2025-03-29T19:27:53.918723Z","shell.execute_reply":"2025-03-29T19:27:53.918587Z"},"id":"Zm7D0L85Qe2C","outputId":"d98d1e78-be94-48f7-f8f8-632233f2806a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport plotly.express as px\n\ntsne_cls = TSNE(n_components=2, random_state=42).fit_transform(emb_ner)\n\nlabels = x['original_lebel']   # Extract labels\nreview_texts = x['Converted Sentence']  # Extract review texts\n\n# Prepare the data for Plotly plots\nplot_data_cls = {'t-SNE Dimension 1': tsne_cls[:, 0],\n                 't-SNE Dimension 2': tsne_cls[:, 1],\n                 'Label': labels,\n                 'Text': review_texts}\n\n# Customize colorbar to show integer labels\ncolorbar_tickvals = list(range(1, 6))\ncolorbar_ticktext = [str(val) for val in colorbar_tickvals]\n\n# Plotting with Plotly - [CLS] Token Embeddings\nfig_cls = px.scatter(plot_data_cls, x='t-SNE Dimension 1', y='t-SNE Dimension 2',\n                     color='Label', hover_data={'t-SNE Dimension 1': False, 't-SNE Dimension 2': False, 'Text': True})\n\n\nfig_cls.update_layout(title='t-SNE of Embeddings NER Converted Training Data with Labels', height=700,  font=dict(\n        size=20,\n))\nfig_cls.update_traces(marker=dict(size=10, opacity=0.9))\n\nfig_cls.update_layout(coloraxis_colorbar=dict(\n    tickvals=colorbar_tickvals,\n    ticktext=colorbar_ticktext\n))\n\nfig_cls.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":717},"execution":{"iopub.status.busy":"2025-03-29T19:27:53.920331Z","iopub.status.idle":"2025-03-29T19:27:53.920903Z","shell.execute_reply":"2025-03-29T19:27:53.920726Z"},"id":"ebvNF-KPQjzC","outputId":"ac928a70-121d-44e3-c967-ae2492a4961c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.to_csv('/content/drive/MyDrive/#Research/# GB/toxicity_reSult_a2.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2025-03-29T19:27:53.921651Z","iopub.status.idle":"2025-03-29T19:27:53.922076Z","shell.execute_reply":"2025-03-29T19:27:53.921879Z"},"id":"X9ioevru5oLX","trusted":true},"outputs":[],"execution_count":null}]}